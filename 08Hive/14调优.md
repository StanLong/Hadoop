# 调优

## 本地模式优化

大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。

### 参数配置

```properties
# 开启本地mr
set hive.exec.mode.local.auto=true;  
# 设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M
set hive.exec.mode.local.auto.inputbytes.max=50000000;
# 设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4
set hive.exec.mode.local.auto.input.files.max=10;
```

### 查询

**未开启本地模式查询**

```sql
0: jdbc:hive2://node01:10000> select count(1) as c1 from emp;
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1612560336369_0001
INFO  : The url to track the job: http://node03:8088/proxy/application_1612560336369_0001/
INFO  : Starting Job = job_1612560336369_0001, Tracking URL = http://node03:8088/proxy/application_1612560336369_0001/
INFO  : Kill Command = /opt/stanlong/hadoop-ha/hadoop-2.9.2/bin/hadoop job  -kill job_1612560336369_0001
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
INFO  : 2021-02-06 06:10:46,580 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-06 06:11:57,264 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-06 06:12:03,030 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 18.73 sec
INFO  : 2021-02-06 06:12:04,113 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 38.26 sec
INFO  : 2021-02-06 06:12:20,278 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 41.3 sec
INFO  : MapReduce Total cumulative CPU time: 41 seconds 300 msec
INFO  : Ended Job = job_1612560336369_0001
+-----+--+
| c1  |
+-----+--+
| 14  |
+-----+--+
1 row selected (149.883 seconds)
```

**开启本地查询**

```sql
0: jdbc:hive2://node01:10000> set hive.exec.mode.local.auto;
+---------------------------------+--+
|               set               |
+---------------------------------+--+
| hive.exec.mode.local.auto=true  |
+---------------------------------+--+
1 row selected (0.012 seconds)
0: jdbc:hive2://node01:10000> set hive.exec.mode.local.auto.inputbytes.max;
+----------------------------------------------------+--+
|                        set                         |
+----------------------------------------------------+--+
| hive.exec.mode.local.auto.inputbytes.max=50000000  |
+----------------------------------------------------+--+
1 row selected (0.009 seconds)
0: jdbc:hive2://node01:10000> set hive.exec.mode.local.auto.input.files.max;
+-----------------------------------------------+--+
|                      set                      |
+-----------------------------------------------+--+
| hive.exec.mode.local.auto.input.files.max=10  |
+-----------------------------------------------+--+
1 row selected (0.036 seconds)
0: jdbc:hive2://node01:10000> select count(1) as c1 from emp;
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_local1159835914_0001 # local
INFO  : The url to track the job: http://localhost:8080/
INFO  : Job running in-process (local Hadoop)               # local Hadoop
INFO  : 2021-02-06 06:14:02,677 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-06 06:14:10,599 Stage-1 map = 100%,  reduce = 0%
INFO  : 2021-02-06 06:14:11,620 Stage-1 map = 100%,  reduce = 100%
INFO  : Ended Job = job_local1159835914_0001
+-----+--+
| c1  |
+-----+--+
| 14  |
+-----+--+
1 row selected (12.037 seconds)
```

## 大表Join大表优化

### 空KEY过滤

有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：

**配置历史服务器**

```xml
配置mapred-site.xml
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>node01:10020</value>
</property>
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>node01:19888</value>
</property>
```

**启动历史服务器**

```shell
mr-jobhistory-daemon.sh start historyserver
```

### 空key转换

有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上

如

```sql
select n.* 
from nullidtable n full join ori o on 
case when n.id is null then concat('hive', rand()) else n.id end = o.id;
```

这种方法消除了数据倾斜，负载均衡reducer的资源消耗

## MapJoin优化

如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理

### 参数配置

```properties
# 设置自动选择Mapjoin
set hive.auto.convert.join = true; 
# 大表小表的阈值设置（默认25M一下认为是小表）
set hive.mapjoin.smalltable.filesize=25000000;
```

![](./doc/09.png)

## Group By 优化

默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。

并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果

### 参数配置

```properties
# 在Map端进行聚合,默认为True
hive.map.aggr = true
# 在Map端进行聚合操作的条目数目
hive.groupby.mapaggr.checkinterval = 100000
# 有数据倾斜的时候进行负载均衡（默认是false）
hive.groupby.skewindata = true
```

当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作

## Count(Distinct) 优化

数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换

## 避免笛卡尔积

## 行列过滤

列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。

行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。应该先对副表进行过滤后再关联主表。

## 动态分区调整

### 参数配置

```properties
# 开启动态分区功能
hive.exec.dynamic.partition=true
# 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区)
hive.exec.dynamic.partition.mode=nonstrict
# 在所有执行MR的节点上，最大一共可以创建多少个动态分区
hive.exec.max.dynamic.partitions=1000
# 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成
大于365，如果使用默认值100，则会报错
hive.exec.max.dynamic.partitions.pernode=100
# 整个MR Job中，最大可以创建多少个HDFS文件
hive.exec.max.created.files=100000
# 当有空分区生成时，是否抛出异常。一般不需要设置
hive.error.on.empty.partition=false
```

## 数据倾斜优化

### 合理设置Map数

1）通常情况下，作业会通过input的目录产生一个或者多个map任务。

主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。

2）是不是map数越多越好？

答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。

3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？

答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。

针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数

#### 小文件进行合并

在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能

```properties
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
```

#### 复杂文件增加Map数

当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。

增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数

### 合理设置Reduce数

reduce个数并不是越多越好

1）过多的启动和初始化reduce也会消耗时间和资源；

2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；

在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适

### 并行执行

Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。

​    通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。

#### 参数配置

```properties
# 打开任务并行执行
set hive.exec.parallel=true;
# 同一个sql允许最大并行度，默认为8。
set hive.exec.parallel.thread.number=16;
```

### 严格模式

Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。

通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询

```xml
<property>
    <name>hive.mapred.mode</name>
    <value>strict</value>
</property>
```

1)    对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。

2)    对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。

3)    限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。

## 执行计划

```sql
0: jdbc:hive2://node01:10000> explain select count(1) as c1 from dwd_emp;
+--------------------------------------------------------------------------------------------------+--+
|                                             Explain                                              |
+--------------------------------------------------------------------------------------------------+--+
| STAGE DEPENDENCIES:                                                                              |
|   Stage-1 is a root stage                                                                        |
|   Stage-0 depends on stages: Stage-1                                                             |
|                                                                                                  |
| STAGE PLANS:                                                                                     |
|   Stage: Stage-1                                                                                 |
|     Map Reduce                                                                                   |
|       Map Operator Tree:                                                                         |
|           TableScan                                                                              |
|             alias: dwd_emp                                                                       |
|             Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE         |
|             Select Operator                                                                      |
|               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE       |
|               Group By Operator                                                                  |
|                 aggregations: count(1)                                                           |
|                 mode: hash                                                                       |
|                 outputColumnNames: _col0                                                         |
|                 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE    |
|                 Reduce Output Operator                                                           |
|                   sort order:                                                                    |
|                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE  |
|                   value expressions: _col0 (type: bigint)                                        |
|       Reduce Operator Tree:                                                                      |
|         Group By Operator                                                                        |
|           aggregations: count(VALUE._col0)                                                       |
|           mode: mergepartial                                                                     |
|           outputColumnNames: _col0                                                               |
|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE          |
|           File Output Operator                                                                   |
|             compressed: false                                                                    |
|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE        |
|             table:                                                                               |
|                 input format: org.apache.hadoop.mapred.TextInputFormat                           |
|                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat        |
|                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                        |
|                                                                                                  |
|   Stage: Stage-0                                                                                 |
|     Fetch Operator                                                                               |
|       limit: -1                                                                                  |
|       Processor Tree:                                                                            |
|         ListSink                                                                                 |
|                                                                                                  |
+--------------------------------------------------------------------------------------------------+--+
```

```sql
0: jdbc:hive2://node01:10000> explain extended select count(1) as c1 from dwd_emp;
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+
|                                                                                                                         Explain                                                                                                                         |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+
| ABSTRACT SYNTAX TREE:                                                                                                                                                                                                                                   |
|                                                                                                                                                                                                                                                         |
| TOK_QUERY                                                                                                                                                                                                                                               |
|    TOK_FROM                                                                                                                                                                                                                                             |
|       TOK_TABREF                                                                                                                                                                                                                                        |
|          TOK_TABNAME                                                                                                                                                                                                                                    |
|             dwd_emp                                                                                                                                                                                                                                     |
|    TOK_INSERT                                                                                                                                                                                                                                           |
|       TOK_DESTINATION                                                                                                                                                                                                                                   |
|          TOK_DIR                                                                                                                                                                                                                                        |
|             TOK_TMP_FILE                                                                                                                                                                                                                                |
|       TOK_SELECT                                                                                                                                                                                                                                        |
|          TOK_SELEXPR                                                                                                                                                                                                                                    |
|             TOK_FUNCTION                                                                                                                                                                                                                                |
|                count                                                                                                                                                                                                                                    |
|                1                                                                                                                                                                                                                                        |
|             c1                                                                                                                                                                                                                                          |
|                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                         |
| STAGE DEPENDENCIES:                                                                                                                                                                                                                                     |
|   Stage-1 is a root stage                                                                                                                                                                                                                               |
|   Stage-0 depends on stages: Stage-1                                                                                                                                                                                                                    |
|                                                                                                                                                                                                                                                         |
| STAGE PLANS:                                                                                                                                                                                                                                            |
|   Stage: Stage-1                                                                                                                                                                                                                                        |
|     Map Reduce                                                                                                                                                                                                                                          |
|       Map Operator Tree:                                                                                                                                                                                                                                |
|           TableScan                                                                                                                                                                                                                                     |
|             alias: dwd_emp                                                                                                                                                                                                                              |
|             Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE                                                                                                                                                                |
|             GatherStats: false                                                                                                                                                                                                                          |
|             Select Operator                                                                                                                                                                                                                             |
|               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE                                                                                                                                                              |
|               Group By Operator                                                                                                                                                                                                                         |
|                 aggregations: count(1)                                                                                                                                                                                                                  |
|                 mode: hash                                                                                                                                                                                                                              |
|                 outputColumnNames: _col0                                                                                                                                                                                                                |
|                 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE                                                                                                                                                           |
|                 Reduce Output Operator                                                                                                                                                                                                                  |
|                   sort order:                                                                                                                                                                                                                           |
|                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE                                                                                                                                                         |
|                   tag: -1                                                                                                                                                                                                                               |
|                   value expressions: _col0 (type: bigint)                                                                                                                                                                                               |
|                   auto parallelism: false                                                                                                                                                                                                               |
|       Needs Tagging: false                                                                                                                                                                                                                              |
|       Reduce Operator Tree:                                                                                                                                                                                                                             |
|         Group By Operator                                                                                                                                                                                                                               |
|           aggregations: count(VALUE._col0)                                                                                                                                                                                                              |
|           mode: mergepartial                                                                                                                                                                                                                            |
|           outputColumnNames: _col0                                                                                                                                                                                                                      |
|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE                                                                                                                                                                 |
|           File Output Operator                                                                                                                                                                                                                          |
|             compressed: false                                                                                                                                                                                                                           |
|             GlobalTableId: 0                                                                                                                                                                                                                            |
|             directory: hdfs://hacluster/tmp/hive/root/12409b41-c78b-429a-b341-1d23dd59a2ee/hive_2021-02-07_05-58-43_474_2576791855329421681-1/-mr-10000/.hive-staging_hive_2021-02-07_05-58-43_474_2576791855329421681-1/-ext-10001                     |
|             NumFilesPerFileSink: 1                                                                                                                                                                                                                      |
|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE                                                                                                                                                               |
|             Stats Publishing Key Prefix: hdfs://hacluster/tmp/hive/root/12409b41-c78b-429a-b341-1d23dd59a2ee/hive_2021-02-07_05-58-43_474_2576791855329421681-1/-mr-10000/.hive-staging_hive_2021-02-07_05-58-43_474_2576791855329421681-1/-ext-10001/  |
|             table:                                                                                                                                                                                                                                      |
|                 input format: org.apache.hadoop.mapred.TextInputFormat                                                                                                                                                                                  |
|                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                                                                                                                                                               |
|                 properties:                                                                                                                                                                                                                             |
|                   columns _col0                                                                                                                                                                                                                         |
|                   columns.types bigint                                                                                                                                                                                                                  |
|                   escape.delim \                                                                                                                                                                                                                        |
|                   hive.serialization.extend.additional.nesting.levels true                                                                                                                                                                              |
|                   serialization.format 1                                                                                                                                                                                                                |
|                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                                                                                                                                                  |
|                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                                                                                                                                                               |
|             TotalFiles: 1                                                                                                                                                                                                                               |
|             GatherStats: false                                                                                                                                                                                                                          |
|             MultiFileSpray: false                                                                                                                                                                                                                       |
|                                                                                                                                                                                                                                                         |
|   Stage: Stage-0                                                                                                                                                                                                                                        |
|     Fetch Operator                                                                                                                                                                                                                                      |
|       limit: -1                                                                                                                                                                                                                                         |
|       Processor Tree:                                                                                                                                                                                                                                   |
|         ListSink                                                                                                                                                                                                                                        |
|                                                                                                                                                                                                                                                         |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+
```

