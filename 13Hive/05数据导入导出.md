# 数据导入导出

## 导入数据

### load 命令

语法：`hive> load data [local] inpath '/opt/module/datas/student.txt' overwrite | into table student [partition (partcol1=val1,…)];`

```
（1）load data:表示加载数据
（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表
（3）inpath:表示加载数据的路径
（4）overwrite:表示覆盖表中已有数据，否则表示追加
（5）into table:表示加载到哪张表
（6）student:表示具体的表
（7）partition:表示上传到指定分区
```

测试数据（默认都是用tab键分隔）

```sql
[root@node01 ~]# vi student.txt
1001	沈万三
1002	朱元璋
1003	陆春香
```

测试表，指定分隔符 \t, 存储格式为 TEXTFILE

```sql
create table student 
(
    id string
   ,name string
)row format delimited fields terminated by '\t' 
STORED AS TEXTFILE;
```

#### load 本地数据到hive表

```sql
hive> load data local inpath '/root/student.txt' into table student;
Loading data to table default.student
Table default.student stats: [numFiles=1, numRows=0, totalSize=39, rawDataSize=0]
OK
Time taken: 2.621 seconds
----------------------------------------------------------------------------------
hive> select * from student;
OK
1001	沈万三
1002	朱元璋
1003	陆春香
Time taken: 0.659 seconds, Fetched: 3 row(s)
```

**这种方式导数据到hive集群多半会报错： Invalid path： No files matching path file**

```sql
hive导入数据语句  load data [local] inpath ，是一个服务器端的指令，它是在服务器端执行。因此指定local时表明加载的文件为本地文件，但是这里的local，在hive中指的是 hiveserver 服务所在的机器，而不是hivecli 或 beeline客户端所在的机器（生产环境大都是 hiveserver 和 hivecli不在同一个机器
```

#### load HDFS上的数据到hive表

先将数据传到HDFS上的任意目录

```shell
[root@node01 ~]# hdfs dfs -put student.txt /
```

再将数据load进表里， **注意这种方式会将HDFS上的原文件删除**

```sql
hive> load data inpath '/student.txt' into table student;
Loading data to table default.student
Table default.student stats: [numFiles=1, totalSize=54]
OK
Time taken: 1.395 seconds
----------------------------------------------------------------------------------
hive> select * from student;
OK
1001    沈万三
1002    朱元璋
1003    陆春香
Time taken: 0.18 seconds, Fetched: 3 row(s)
```

#### load HDFS数据到分区表

创建一张分区表

```sql
create table stu_pt 
(
    id String
   ,name String
)
partitioned by (pt_d String)
row format delimited fields terminated by '\t' 
STORED AS TEXTFILE;
```

load 数据到表分区里, **注意这种方式会将HDFS上的原文件删除**

```sql
hive> load data inpath '/student.txt' into table stu_pt partition (pt_d = '20210129');
Loading data to table default.stu_pt partition (pt_d=20210129)
Partition default.stu_pt{pt_d=20210129} stats: [numFiles=1, numRows=0, totalSize=54, rawDataSize=0]
OK
Time taken: 1.745 seconds
----------------------------------------------------------------------------------
hive> select * from stu_pt where pt_d = '20210129';
OK
1001    沈万三	20210129
1002    朱元璋	20210129
1003    陆春香	20210129
Time taken: 1.906 seconds, Fetched: 3 row(s)
```

```sql
# 如果是多级分区分区字段用,隔开如
create table stu_pt 
(
    id String
   ,name String
)
partitioned by (pt_m String,pt_d String)
row format delimited fields terminated by '\t' 
STORED AS TEXTFILE;
load data inpath '/student.txt' into table stu_pt partition (pt_m='202101',pt_d = '20210129');
```

### put 命令

上传数据到表所在的HDFS路径

```
hive> show create table student; 
LOCATION
  'hdfs://hacluster/user/hivedb/warehouse/student'
[root@node01 ~]# hdfs dfs -put -f student.txt /user/hivedb/warehouse/student
```

查看数据

```sql
hive> select * from student;
OK
1001	沈万三
1002	朱元璋
1003	陆春香
Time taken: 0.254 seconds, Fetched: 3 row(s)
```

### 查询导入

#### 非分区表

```sql
create table if not exists student2 as select id, name from student;
```

#### 分区表

基本插入模式

```sql
insert overwrite table stu_pt partition(pt_d='20210129')
select id, name from student where id='1001';
```

多插入模式

```sql
from student
insert overwrite table stu_pt partition(pt_d='20210127')
select id, name where id='1002'
insert overwrite table stu_pt partition(pt_d='20210128')
select id, name where id='1003';
```

### Import 命令

import 导入的数据一定得是用export导出的，包含元数据信息。 import 导入的表得是空表

```sql
0: jdbc:hive2://node01:10000> import table stu_pt01 from '/stu_pt';
INFO  : Copying data from hdfs://hacluster/stu_pt/pt_d=20210127 to hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10000
INFO  : Copying file: hdfs://hacluster/stu_pt/pt_d=20210127/000000_0
INFO  : Copying data from hdfs://hacluster/stu_pt/pt_d=20210128 to hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10001
INFO  : Copying file: hdfs://hacluster/stu_pt/pt_d=20210128/000000_0
INFO  : Copying data from hdfs://hacluster/stu_pt/pt_d=20210129 to hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10002
INFO  : Copying file: hdfs://hacluster/stu_pt/pt_d=20210129/000000_0
INFO  : Loading data to table default.stu_pt01 partition (pt_d=20210127) from hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10000
INFO  : Loading data to table default.stu_pt01 partition (pt_d=20210128) from hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10001
INFO  : Loading data to table default.stu_pt01 partition (pt_d=20210129) from hdfs://hacluster/stu_pt/.hive-staging_hive_2021-01-30_21-16-18_003_3635285683741515670-1/-ext-10002
No rows affected (3.283 seconds)
0: jdbc:hive2://node01:10000> select * from  stu_pt01;
+--------------+----------------+----------------+--+
| stu_pt01.id  | stu_pt01.name  | stu_pt01.pt_d  |
+--------------+----------------+----------------+--+
| 1002         | 朱元璋            | 20210127       |
| 1003         | 陆春香            | 20210128       |
| 1001         | 沈万三            | 20210129       |
+--------------+----------------+----------------+--+
```

## 导出数据

### **insert导出**

注意这种导出方式只能在 hive cli 执行， 用beeline 执行没有效果

- 将查询的结果导出到本地

```sql
hive (default)> insert overwrite local directory '/opt/module/datas/export/student'
            select * from student;
```

- 将查询的结果格式化导出到本地

```sql
hive(default)>insert overwrite local directory '/opt/module/datas/export/student1'
           ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'             select * from student;
```

- 将查询的结果导出到HDFS上(没有local)

```sql
hive (default)> insert overwrite directory '/user/atguigu/student2'
             ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
             select * from student;
```

### Hadoop命令导出

方式一

```sql
hive (default)> dfs -get /user/hive/warehouse/student/month=201709/000000_0
/opt/module/datas/export/student3.txt;
```

方式二

```shell
[root@node02 ~]# hdfs dfs -getmerge /user/hivedb/warehouse/city_info /root/city_info.csv
[root@node02 ~]# hdfs dfs -getmerge /user/hivedb/warehouse/city_info /root/city_info.txt
```

当导出的数据文件格式为 csv 时，可能会出现数据乱码

原因有下面两个：

1.乱码的原因是用excel打开csv时格式默认为gbk，但是从hive中导出来的是utf8的

2.格式不对的原因是csv文件的列分隔符是逗号或者\t，而hive中默认使用\001

需要使用 ` iconv -f UTF-8 -c  -t GBK testaa.csv > testbb.csv` 转换编码

### beeline命令导出

```sql
[atguigu@hadoop102 hive]$ beeline -e 'select * from default.student;' >
 /opt/module/datas/export/student4.txt;
```

```shell
// 如果查询的数据量特别大，在执行查询前可以先指定内存
export HADOOP_CLIENT_OPTS="-Xmx5120M"
// 指定导出文件的分隔符
beeline --outputformat=dsv --delimiterForDsv="|" -f query.sql > result.csv
```



### Export命令

导出目录必须为空

```sql
0: jdbc:hive2://node01:10000> export table stu_pt to '/stu_pt';
INFO  : Copying data from file:/tmp/root/5c9977b4-6b0d-4d45-a445-53df9cf96bc7/hive_2021-01-30_21-08-22_109_4391445748643052853-1/-local-10000/_metadata to hdfs://hacluster/stu_pt
INFO  : Copying file: file:/tmp/root/5c9977b4-6b0d-4d45-a445-53df9cf96bc7/hive_2021-01-30_21-08-22_109_4391445748643052853-1/-local-10000/_metadata
INFO  : Copying data from hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210127 to hdfs://hacluster/stu_pt/pt_d=20210127
INFO  : Copying file: hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210127/000000_0
INFO  : Copying data from hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210128 to hdfs://hacluster/stu_pt/pt_d=20210128
INFO  : Copying file: hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210128/000000_0
INFO  : Copying data from hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210129 to hdfs://hacluster/stu_pt/pt_d=20210129
INFO  : Copying file: hdfs://hacluster/user/hivedb/warehouse/stu_pt/pt_d=20210129/000000_0
No rows affected (1.721 seconds)
```

