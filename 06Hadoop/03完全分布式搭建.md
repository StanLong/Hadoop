# Hadoop2.x完全分布式搭建

## 一、集群规划

| 主机   | NN   | SNN  | DN   |
| ------ | ---- | ---- | ---- |
| node01 | NN   |      |      |
| node02 |      | SNN  | DN   |
| node03 |      |      | DN   |

NN 基于内存存储，对内存要求比较高

## 二、环境检查

安装jdk， 检查 /etc/hosts/， 做ssh免密，检查主机名，检查时间是否同步

## 三、hadoop部署

### 1、解压安装

```shell
# 解压
[root@node01 ~]# tar -zxf hadoop-2.9.2.tar.gz -C /opt/stanlong/hadoop-cluster/
```

### 2、配置hadoop环境变量

```shell
[root@node01 hadoop-2.9.2]# vi /etc/profile

78 # export HADOOP_HOME=/opt/stanlong/hadoop-single/hadoop-2.9.2 # HADOOP单节点环境变量
79 export HADOOP_HOME=/opt/stanlong/hadoop-cluster/hadoop-2.9.2 # HADOOP完全分布式环境变量
80 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 使环境变量生效
[root@node01 bin]# source /etc/profile

# 输入ha能看到命令提示说明环境变量配置成功
[root@node01 bin]# ha
hadoop             hadoop.cmd         hadoop-daemon.sh   hadoop-daemons.sh  halt               hardlink           hash      
```

### 3、java环境变量二次配置

```shell
[root@node01 hadoop]# pwd
/opt/stanlong/hadoop-cluster/hadoop-2.9.2/etc/hadoop
[root@node01 hadoop]# vi hadoop-env.sh 
  24 # The java implementation to use.
  25 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi mapred-env.sh 
  16 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi yarn-env.sh
  23 export JAVA_HOME=/usr/java/jdk1.8.0_65
```

### 4、编辑 core-site.xml(NN)

```shell
[root@node01 hadoop]# vi core-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!-- 规划了namenode在哪启动 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
    <!-- 配置NN数据存放路径,目录必须为空 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/data/hadoop/full</value>
    </property>
</configuration>
```

### 5、编辑 hdfs-site.xml(SN)

```shell
[root@node01 hadoop]# vi hdfs-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <!-- 规划了secondaryName在哪启动 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node02:50090</value>
    </property>
</configuration>
```

### 6、编辑 slaves（DN）

```shell
[root@node01 hadoop]# vi slaves # slaves 规划了DataNode 在哪启动
# 把文件中的 localhost 替换成 node02 node03
node02
node03
```

### 7、分发hadoop

把 hadoop 配置分发到 node02，node03上去

分发脚本参考 23自定义集群脚本/分发脚本

```shell
[root@node01 myshell]# ./rsyncd.sh /opt/stanlong/hadoop-cluster/hadoop-2.9.2
```

### 7、分发/etc/profile文件

```shell
[root@node01 myshell]# ./rsyncd.sh /etc/profile
```

使node02和node03上的profile文件生效

```shell
[root@node02 ~]# source /etc/profile
```

## 四、运行

### 1、namenode格式化

```shell
[root@node01 hadoop]# hdfs namenode -format
# 当看到 common.Storage: Storage directory /var/data/hadoop/full/dfs/name has been successfully formatted 说明格式化成功
```

如果执行 格式化命令时发现命令仍格式化 **/var/data/hadoop/local/dfs/name**， 说明当前连接窗口中出现了hash缓存，关闭当前窗口，重新开一个即可

### 2、启动dfs

```shell
[root@node01 ~]# start-dfs.sh 
Starting namenodes on [node01]
node01: starting namenode, logging to /opt/stanlong/hadoop-cluster/hadoop-2.9.2/logs/hadoop-root-namenode-node01.out
node03: starting datanode, logging to /opt/stanlong/hadoop-cluster/hadoop-2.9.2/logs/hadoop-root-datanode-node03.out
node02: starting datanode, logging to /opt/stanlong/hadoop-cluster/hadoop-2.9.2/logs/hadoop-root-datanode-node02.out
Starting secondary namenodes [node02]
node02: starting secondarynamenode, logging to /opt/stanlong/hadoop-cluster/hadoop-2.9.2/logs/hadoop-root-secondarynamenode-node02.out
```

### 3、查看各节点的进程

```shell
[root@node01 ~]# jps
4118 Jps
3737 NameNode

[root@node02 ~]# jps
3616 Jps
3527 SecondaryNameNode
3467 DataNode

[root@node03 ~]# jps
3401 DataNode
3481 Jps
```

### 4、网页访问

http://node01:50070/dfshealth.html#tab-datanode

![](./doc/07.png)

# Hadoop3.x完全分布式搭建

集群规划与环境检查同上

## 一、hadoop部署

### 1、解压安装

```shell
tar -zxf hadoop-3.4.0.tar.gz -C /opt
```

### 2、配置hadoop环境变量

```shell
[root@node01 ~]# vi .bashrc
export HADOOP_HOME=/opt/hadoop-3.4.0
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
[root@node01 ~]# source .bashrc
[root@node01 ~]# ha
hadoop             hadoop-daemon.sh   halt               hash               
hadoop.cmd         hadoop-daemons.sh  hardlink           
```

### 3、hadoop配置Java环境

```shell
[root@node01 ~]# cd $HADOOP_HOME/etc/hadoop
[root@node01 hadoop]# vi hadoop-env.sh 
# 配置JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/jdk-1.8.0_421-oracle-x64

# 在  # export HDFS_NAMENODE_USER=hdfs 添加如下几行， 通过使用export HDFS_NAMENODE_USER=hdfs来限制哪个用户可以执行namenode命令
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

### 4、编辑 core-site.xml(NN)

```xml
<configuration>
    <!-- 规划了namenode在哪启动 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
    <!-- 配置NN数据存放路径,目录必须为空 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/data/hadoop/full</value>
    </property>
</configuration>
```

### 5、编辑 hdfs-site.xml(SN)

```xml
<configuration>
    <!-- 规划了1个副本数 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!-- 规划了secondaryName在哪启动 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:9869</value>
    </property>
</configuration>
```

### 6、编辑 worker（DN）

```shell
# node02 和 node03 做为datanode
sed -i '/localhost/d' workers

cat > workers << EOF
node02
node03
EOF
```

### 7、分发

在node01上配置好 hadoop之后，将hadoop安装包和相关环境变量分发到node02和node03上

## 二、运行

namenode格式化与启动dfs和hadoop2.x 相同，但是网页访问的时候端口号变了

http://192.168.235.11:9870/explorer.html#/user/root

