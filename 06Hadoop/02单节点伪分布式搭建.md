# Hadoop2.X单节点伪分布式搭建

## 一、节点规划

| 节点   | 进程        |
| ------ | ----------- |
| node01 | NN，DN，SNN |

## 二、环境准备

1. 检查/etc/hosts文件,  集群中主机名和IP地址映射是否正确

2. 检查hostname

   ```shell
   [root@node01 stanlong]# hostname
   node01
   [root@node01 stanlong]# cat /etc/sysconfig/network
   # Created by anaconda
   HOSTNAME=node01
   ```
   
3. 检查集群时间是否一致

4. 安装jdk并配置环境变量

5. 配置单机免密钥登录

   ```shell
   ssh-keygen -t dsa -P '' -f /root/.ssh/id_dsa
   cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
   ```

## 三、Hadoop伪分布式搭建

### 1、解压安装包

```shell
# 为方便区分不同的安装部署模式，这里解压到 /opt/stanlong/hadoop-single/ 目录
[root@node01 ~]# tar -zxf  hadoop-2.9.2.tar.gz -C /opt/stanlong/hadoop-single/
```

### 2、配置 hadoop环境变量

```shell
[root@node01 bin]# vi /etc/profile

# 在文件末尾添加HADOOP环境变量
export HADOOP_HOME=/opt/stanlong/hadoop-single/hadoop-2.9.2 # HADOOP单节点环境变量
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 使环境变量生效
[root@node01 bin]# source /etc/profile

# 输入ha能看到命令提示说明环境变量配置成功
[root@node01 bin]# ha
hadoop             hadoop.cmd         hadoop-daemon.sh   hadoop-daemons.sh  halt               hardlink           hash      
```

### 3、hadoop配置java环境

```shell
[root@node01 hadoop]# pwd
/opt/stanlong/hadoop-single/hadoop-2.9.2/etc/hadoop  # hadoop 文件的配置目录
[root@node01 hadoop]# vi hadoop-env.sh 
  24 # The java implementation to use.
  25 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi mapred-env.sh 
  16 export JAVA_HOME=/usr/java/jdk1.8.0_65
[root@node01 hadoop]# vi yarn-env.sh
  23 export JAVA_HOME=/usr/java/jdk1.8.0_65
```

### 4、编辑 core-site.xml(NN)

```shell
[root@node01 hadoop]# vi core-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!-- 规划了namenode在哪启动 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
    <!-- 配置NN数据存放路径,目录必须为空 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/data/hadoop/local</value>
    </property>
</configuration>
```

### 5、编辑 hdfs-site.xml(SN)

```shell
[root@node01 hadoop]# vi hdfs-site.xml
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!-- 规划了1个副本数 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!-- 规划了secondaryName在哪启动 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:50090</value>
    </property>
</configuration>
```

### 6、编辑 slaves（DN）

```shell
 # slaves 规划了DataNode 在哪启动
[root@node01 hadoop]# echo 'node01' > slaves
```

## 四、运行

### 1、namenode格式化

```shell
[root@node01 hadoop]# hdfs namenode -format
# 当看到 common.Storage: Storage directory /var/data/hadoop/local/dfs/name has been successfully formatted. 说明格式化成功
```

格式化成功之后在 /var/data/hadoop/local/dfs/ 会生成相应的fsimage文件

```shell
[root@node01 current]# cd /var/data/hadoop/local/dfs/name/current
[root@node01 current]# ll
total 16
-rw-r--r-- 1 root root 399 2024-08-04 23:45:28 fsimage_0000000000000000000
-rw-r--r-- 1 root root  62 2024-08-04 23:45:28 fsimage_0000000000000000000.md5
-rw-r--r-- 1 root root   2 2024-08-04 23:45:28 seen_txid
-rw-r--r-- 1 root root 218 2024-08-04 23:45:28 VERSION
```

### 2、启动 dfs

```shell
[root@node01 current]# start-dfs.sh 
Starting namenodes on [node01]
node01: starting namenode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-namenode-node01.out
node01: starting datanode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-datanode-node01.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/stanlong/hadoop-2.9.2/logs/hadoop-root-secondarynamenode-node01.out
```

启动成功后观察  /var/data/hadoop/local/dfs/

```shell
[root@node01 dfs]# cd /var/data/hadoop/local/dfs
[root@node01 dfs]# ll
total 0
drwx------ 3 root root 40 Jan 15 07:34 data # DN 生成的目录，如果是分布式，会出现在对应的机器上
drwxr-xr-x 3 root root 40 Jan 15 07:34 name # NN 生成的目录
drwxr-xr-x 3 root root 40 Jan 15 07:35 namesecondary # SN 生成的目录如果是分布式，会出现在对应的机器上
[root@node01 dfs]# 
```

```shell
# 启动成功后会看到如下进程，在hadoop中角色即进程
[root@node01 current]# jps
21520 Jps
21335 SecondaryNameNode
21162 DataNode
21067 NameNode
```

### 3、网页访问

http://node01:50070/dfshealth.html#tab-overview

### 4、停止dfs

```shell
[root@node01 current]# stop-dfs.sh 
Stopping namenodes on [node01]
node01: stopping namenode
node01: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
```

# Hadoop3.X单节点伪分布式搭建

节点规划 和 环境准备 与2.x相同

## 一、Hadoop伪分布式搭建

### 1、解压安装包

```shell
tar -zxf  hadoop-3.4.0.tar.gz -C /opt/
```

### 2、配置 hadoop环境变量

```shell
cat ~/.bashrc
export HADOOP_HOME=/opt/hadoop-3.4.0
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

### 3、hadoop配置java环境

```shell
[root@node01 ~]# cd $HADOOP_HOME/etc/hadoop
[root@node01 hadoop]# vi hadoop-env.sh 
# 配置JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/jdk-1.8.0_421-oracle-x64

# 在  # export HDFS_NAMENODE_USER=hdfs 添加如下几行， 通过使用export HDFS_NAMENODE_USER=hdfs来限制哪个用户可以执行namenode命令
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

### 4、编辑 core-site.xml(NN)

```xml
<configuration>
    <!-- 规划了namenode在哪启动 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
    <!-- 配置NN数据存放路径,目录必须为空 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/data/hadoop/local</value>
    </property>
</configuration>
```

### 5、编辑 hdfs-site.xml(SN)

```xml
<configuration>
    <!-- 规划了1个副本数 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!-- 规划了secondaryName在哪启动 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:9869</value>
    </property>
</configuration>
```

### 6、编辑 worker（DN）

```shell
echo 'node01' > workers
```

## 二、运行

namenode格式化与启动dfs和hadoop2.x 相同，但是网页访问的时候端口号变了

http://192.168.235.11:9870/explorer.html#/user/root

## 附录：hdfs文件操作命令

- 新建目录

  ```shell
  [root@node01 ~]# hdfs dfs -mkdir -p /user/root
  ```

- 上传文件

  ```shell
  [root@node01 ~]# hdfs dfs -put hadoop-3.4.0.tar.gz  /user/root
  ```

- 指定上传文件的块大小

  ```shell
  [root@node01 ~]# hdfs dfs -D dfs.blocksize=1048576 -put tengine-2.1.0.tar.gz
  ```


hdfs 相关参数配置地址

https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml





